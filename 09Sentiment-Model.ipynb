{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 極性予測モデルを作る"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要なもの\n",
    "* ID、POPULATION_IDのセット\n",
    "* 04で作成したテキストのみファイルを、母集団、分析対象それぞれOnly_Textsフォルダーに入れる。（ファイル名はID.txt, POPULATIONID.txt）\n",
    "* 05で作成した極性教師データを母集団、分析対象それぞれSentiment-Teacher/ID.txtに入れる\n",
    "* PRIORITYの調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = \"\"\n",
    "\n",
    "POPULATION_ID = \"\"\n",
    "\n",
    "#分析対象テキストデータ\n",
    "target_texts_file = \"Only_Texts/%s.txt\"%ID\n",
    "\n",
    "#母集団テキストデータ\n",
    "population_texts_file = \"Only_Texts/%s.txt\"%POPULATION_ID\n",
    "\n",
    "#分析対象の極性教師データ\n",
    "target_teacher_file = \"Sentiment-Teacher/%s.txt\"%ID\n",
    "\n",
    "#母集団の極性教師データ\n",
    "population_teacher_file = \"Sentiment-Teacher/%s.txt\"%POPULATION_ID\n",
    "\n",
    "#アマゾンの極性教師データ\n",
    "amazon_teacher_file_pos = \"Amazon/positive.txt\"\n",
    "amazon_teacher_file_neg = \"Amazon/negative.txt\"\n",
    "\n",
    "#TFIDFモデルの保存\n",
    "tfidf_model_file = \"Models/Tfidf/%s-from-%s.model\"%(ID, POPULATION_ID)\n",
    "\n",
    "#分析器の保存\n",
    "classifer_model_file = \"Models/Classifers/%s-from-%s.model\"%(ID, POPULATION_ID)\n",
    "\n",
    "#最適化にあたるスコアの各データの比率（どのデータの精度を優先するか）\n",
    "#分析対象データの過学習を避けるため\n",
    "TARGET_PRIORITY = 0.5\n",
    "POPULATION_PRIORITY = 0.3\n",
    "AMAZON_PRIORITY = 0.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ファイル読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストデータ群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_texts = pd.read_table(target_texts_file, names=[\"Texts\"])\n",
    "df_population_texts = pd.read_table(population_texts_file, names=[\"Texts\"])\n",
    "\n",
    "\n",
    "df_target_texts.shape, df_population_texts.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 教師データ群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.read_table(target_teacher_file, names=[\"Texts\", \"Sentiment\"], index_col=None)\n",
    "\n",
    "df_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_population = pd.read_table(population_teacher_file, names=[\"Texts\", \"Sentiment\"], index_col=None)\n",
    "\n",
    "df_population.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon_pos = pd.read_table(amazon_teacher_file_pos, names=[\"Texts\"], index_col=None)\n",
    "df_amazon_neg = pd.read_table(amazon_teacher_file_neg, names=[\"Texts\"], index_col=None)\n",
    "\n",
    "df_amazon_texts = pd.concat([df_amazon_pos, df_amazon_neg])\n",
    "\n",
    "df_amazon_pos[\"Sentiment\"] = \"pos\"\n",
    "df_amazon_neg[\"Sentiment\"] = \"neg\"\n",
    "\n",
    "df_amazon = pd.concat([df_amazon_pos, df_amazon_neg])\n",
    "\n",
    "df_amazon_texts.shape, df_amazon.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全テキストデータの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_texts = pd.concat([df_target_texts, df_population_texts, df_amazon_texts])\n",
    "\n",
    "df_all_texts.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データと評価データの切り分け"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "#各データの重みづけ\n",
    "TARGET_WEIGHT       = 5\n",
    "POPULATION_WEIGHT   = 3\n",
    "AMAZON_WEIGHT       = 1\n",
    "\n",
    "#切り分け\n",
    "df_target_train, df_target_valid = train_test_split(df_target, test_size=TEST_RATIO, random_state=334)\n",
    "df_population_train, df_population_valid = train_test_split(df_population, test_size=TEST_RATIO, random_state=334)\n",
    "df_amazon_train, df_amazon_valid = train_test_split(df_amazon, test_size=TEST_RATIO, random_state=334)\n",
    "\n",
    "df_all_train = pd.concat([df_target_train, df_population_train, df_amazon_train])\n",
    "df_all_valid = pd.concat([df_target_valid, df_population_valid, df_amazon_valid])\n",
    "\n",
    "df_all = pd.concat([df_all_train, df_all_valid])\n",
    "\n",
    "#重みづけ配列\n",
    "data_weights = []\n",
    "def get_wights(target_weight, population_weight, amazon_weight):\n",
    "    data_weights = []\n",
    "\n",
    "    for _ in range(len(df_target_train)):\n",
    "        data_weights.append(target_weight)\n",
    "    for _ in range(len(df_population_train)):\n",
    "        data_weights.append(population_weight)\n",
    "    for _ in range(len(df_amazon_train)):\n",
    "        data_weights.append(amazon_weight)\n",
    "\n",
    "    return data_weights\n",
    "\n",
    "data_weights = get_wights(5,3,1)\n",
    "\n",
    "df_all_train.shape, df_all_valid.shape, len(data_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDFモデルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#形態素分解＋原形をリスト化\n",
    "def tokenize(text):\n",
    "    output = []\n",
    "    \n",
    "    tagger = MeCab.Tagger()\n",
    "    node = tagger.parseToNode(text)\n",
    "\n",
    "    while node:\n",
    "        #原形を記録\n",
    "        features = node.feature.split(\",\")\n",
    "        if len(features) >= 8:\n",
    "            output.append(features[7])\n",
    "        else:\n",
    "            output.append(node.surface)\n",
    "\n",
    "        node = node.next\n",
    "    \n",
    "    return output\n",
    "\n",
    "print(tokenize(\"オレンジ今日も食べてみたけどまだ酸っぱくて泣いた\"))\n",
    "\n",
    "#単語ベクトル作成+TFIDFモデルの作成\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1,3), min_df=20, sublinear_tf=True)\n",
    "tfidf_fit = vectorizer.fit(df_all_texts[\"Texts\"].values.astype(\"U\"))\n",
    "\n",
    "#各データフレームをTFIDFのspycy行列に\n",
    "num_target_train     = tfidf_fit.transform(df_target_train[\"Texts\"].values.astype(\"U\"))\n",
    "num_target_valid     = tfidf_fit.transform(df_target_valid[\"Texts\"].values.astype(\"U\"))\n",
    "num_population_train = tfidf_fit.transform(df_population_train[\"Texts\"].values.astype(\"U\"))\n",
    "num_population_valid = tfidf_fit.transform(df_population_valid[\"Texts\"].values.astype(\"U\"))\n",
    "num_amazon_train     = tfidf_fit.transform(df_amazon_train[\"Texts\"].values.astype(\"U\"))\n",
    "num_amazon_valid     = tfidf_fit.transform(df_amazon_valid[\"Texts\"].values.astype(\"U\"))\n",
    "\n",
    "num_all_train        = tfidf_fit.transform(df_all_train[\"Texts\"].values.astype(\"U\"))\n",
    "num_all_valid        = tfidf_fit.transform(df_all_valid[\"Texts\"].values.astype(\"U\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロジスティック回帰による分類モデルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spycy行列を普通の行列に\n",
    "x_train             = num_all_train.toarray()\n",
    "x_valid             = num_all_valid.toarray()    \n",
    "x_target_valid      = num_target_valid.toarray()\n",
    "x_population_valid  = num_population_valid.toarray()\n",
    "x_amazon_valid      = num_amazon_valid.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分類モデル\n",
    "classifer = SGDClassifier(loss=\"log\", penalty=\"l2\", random_state=334)\n",
    "\n",
    "#最適化スコアを取得\n",
    "def get_total_score(classifer : SGDClassifier):\n",
    "    amazon_score = classifer.score(x_amazon_valid,          df_amazon_valid[\"Sentiment\"])\n",
    "    population_score = classifer.score(x_population_valid,  df_population_valid[\"Sentiment\"])\n",
    "    target_score = classifer.score(x_target_valid,          df_target_valid[\"Sentiment\"])\n",
    "\n",
    "    return amazon_score*AMAZON_PRIORITY + population_score*POPULATION_PRIORITY + target_score*TARGET_PRIORITY\n",
    "\n",
    "#学習・最適なデータ重み比を走査\n",
    "best = None\n",
    "max_score = 0\n",
    "for target in range(0,5,1):\n",
    "    for population in range(0,5,1):\n",
    "        for amazon in range(0,5,1):\n",
    "            data_weights = get_wights(target, population, amazon)\n",
    "            classifer = SGDClassifier(loss=\"log\", penalty=\"l2\", random_state=334)\n",
    "            classifer.fit(x_train, df_all_train[\"Sentiment\"], sample_weight=data_weights)\n",
    "            score = get_total_score(classifer)\n",
    "            if max_score < score:\n",
    "                best = (target, population, amazon)\n",
    "                max_score = score\n",
    "\n",
    "classifer = SGDClassifier(loss=\"log\", penalty=\"l2\", random_state=334)\n",
    "data_weights = get_wights(best[0], best[1], best[2])\n",
    "classifer.fit(x_train, df_all_train[\"Sentiment\"], sample_weight=data_weights)\n",
    "\n",
    "best, max_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#評価データ\n",
    "target_score        = classifer.score(x_target_valid,       df_target_valid[\"Sentiment\"])\n",
    "population_score    = classifer.score(x_population_valid,   df_population_valid[\"Sentiment\"])\n",
    "amazon_score        = classifer.score(x_amazon_valid,       df_amazon_valid[\"Sentiment\"])\n",
    "\n",
    "target_score, population_score, amazon_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "predicted = classifer.predict(x_target_valid)\n",
    "\n",
    "conf_mat = confusion_matrix(df_target_valid[\"Sentiment\"], predicted)\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\",\n",
    "                xticklabels=classifer.classes_, yticklabels=classifer.classes_)\n",
    "\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#TFIDFモデル\n",
    "with open(tfidf_model_file, \"wb\") as f:\n",
    "    pickle.dump(tfidf_fit, f)\n",
    "\n",
    "#分類モデル\n",
    "with open(classifer_model_file, \"wb\") as f:\n",
    "    pickle.dump(classifer, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af19612f8f38f6812d87a5ef9419a460d9700d7e1cf8ff071cf581e2ee4d6233"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
