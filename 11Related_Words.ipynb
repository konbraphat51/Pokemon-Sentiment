{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11で出した重要語と共起する単語を算出"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要なもの\n",
    "* ID, POPULATION_IDのセット\n",
    "* local_stopwordsのセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = \"\"\n",
    "\n",
    "POPULATION_ID = \"\"\n",
    "\n",
    "#10の途中経過\n",
    "without_stopwords_text_file = \"Progresses/NonStopword/%s-from-%s.txt\"%(ID, POPULATION_ID)\n",
    "\n",
    "#10の結果\n",
    "sentiment_file = \"Sentiment/%s-from-%s.txt\"%(ID, POPULATION_ID)\n",
    "\n",
    "##TFIDFモデル\n",
    "tfidf_model_file = \"Models/Tfidf/%s-from-%s.model\"%(ID, POPULATION_ID)\n",
    "\n",
    "#内部的に品詞を区別する区切り文字\n",
    "TOKEN_DIVIDER = \"<334>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ここだけのストップワード\n",
    "#投稿そのものを除外するのではなく、関連単語から除外する\n",
    "local_stopwords = [\"ミミッキュ\", \"br\", \"ミミ\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.read_table(without_stopwords_text_file)\n",
    "\n",
    "#本ノートでは生テキストではなく形態素解析後のテキストが渡される\n",
    "def tokenize(text):\n",
    "    return text.split(TOKEN_DIVIDER)\n",
    "\n",
    "df_main.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.read_table(sentiment_file)\n",
    "\n",
    "df_sentiment.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単純な共起回数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語ベクトルの用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tokenize, stop_words=local_stopwords, ngram_range=(1,1), min_df=20)\n",
    "vectorizer = vectorizer.fit(df_main[\"disassembled_target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywordに共起する回数の多いものを数える\n",
    "def get_freq(keyword):\n",
    "    #keywordを含む行を走査\n",
    "    df_keyword = np.empty((0, df_main.shape[1]))\n",
    "    for _, row in df_main.iterrows():\n",
    "        if keyword in tokenize(row[\"disassembled_all\"]):\n",
    "            df_keyword = np.vstack([df_keyword, row.values])\n",
    "\n",
    "    #pandas行列に\n",
    "    df_keyword = pd.DataFrame(df_keyword, columns=df_main.columns)\n",
    "\n",
    "    #単語ベクトルの回数数える\n",
    "    freq_spycy = vectorizer.transform(df_keyword[\"disassembled_target\"])\n",
    "    freq_df = pd.DataFrame(freq_spycy.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    freq_sum = freq_df.sum(axis=0)\n",
    "    \n",
    "    #単語名と結び付け\n",
    "    freq_sum = freq_sum.sort_values(ascending=False)\n",
    "\n",
    "    #ストップワードの削除\n",
    "    for word in local_stopwords:\n",
    "        try:\n",
    "            freq_sum = freq_sum.drop(word)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return freq_sum\n",
    "\n",
    "get_freq(\"良い\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment[\"CoOccurre-FREQ\"] = \"\"\n",
    "for index, row in df_sentiment.iterrows():\n",
    "    freq = get_freq(row[\"Token\"])\n",
    "    end = 6\n",
    "    if len(freq) <= 1:\n",
    "        end = 1\n",
    "    elif len(freq) < 6:\n",
    "        end = len(freq)\n",
    "\n",
    "    if end != 1:\n",
    "        cooc = \", \".join(list(freq.iloc[1:end].index))\n",
    "        df_sentiment.at[index, \"CoOccurre-FREQ\"] = cooc\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "df_sentiment.head(100).style.background_gradient(cmap=\"vlag_r\", axis=0, subset=\"Sentiment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析対象内"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 母集団に対するTFIDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 母集団TFIDFモデルをサルベージ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "with open(tfidf_model_file, \"rb\") as f:\n",
    "    tfidf_model = pickle.load(f)\n",
    "\n",
    "#対象（名詞・形容詞）を見せる用\n",
    "tfidf_model_showing = deepcopy(tfidf_model)\n",
    "tfidf_model_showing.ngram = (1,1)\n",
    "tfidf_model_showing.tokenizer = tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDFを計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywordに共起する回数の多いものを数える\n",
    "def get_tfidf(keyword):\n",
    "    #keywordを含む行を走査\n",
    "    df_keyword = np.empty((0, df_main.shape[1]))\n",
    "    for _, row in df_main.iterrows():\n",
    "        if keyword in tokenize(row[\"disassembled_all\"]):\n",
    "            df_keyword = np.vstack([df_keyword, row.values])\n",
    "\n",
    "    #pandas行列に\n",
    "    df_keyword = pd.DataFrame(df_keyword, columns=df_main.columns)\n",
    "\n",
    "    if df_keyword.shape[0] == 0:\n",
    "        return pd.Series()\n",
    "\n",
    "    #TFIDF計算\n",
    "    tfidf_spycy = tfidf_model_showing.transform(df_keyword[\"disassembled_target\"])\n",
    "    tfidf_df = pd.DataFrame(tfidf_spycy.toarray(), columns=tfidf_model_showing.get_feature_names_out())\n",
    "    tfidf_mean = tfidf_df.mean(axis=0)\n",
    "    \n",
    "    #単語名と結び付け\n",
    "    tfidf_mean = tfidf_mean.sort_values(ascending=False)\n",
    "\n",
    "    #ストップワードの削除\n",
    "    for word in local_stopwords:\n",
    "        try:\n",
    "            tfidf_mean = tfidf_mean.drop(word)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return tfidf_mean\n",
    "\n",
    "get_tfidf(\"良い\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment[\"CoOccurre-TFIDF\"] = \"\"\n",
    "for index, row in df_sentiment.iterrows():\n",
    "    freq = get_tfidf(row[\"Token\"])\n",
    "    end = 6\n",
    "    if len(freq) <= 1:\n",
    "        end = 1\n",
    "    elif len(freq) < 6:\n",
    "        end = len(freq)\n",
    "\n",
    "    if end != 1:\n",
    "        cooc = \", \".join(list(freq.iloc[1:end].index))\n",
    "        df_sentiment.at[index, \"CoOccurre-TFIDF\"] = cooc\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "df_sentiment.head(200).style.background_gradient(cmap=\"vlag_r\", axis=0, subset=\"Sentiment\", vmin=-1, vmax=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af19612f8f38f6812d87a5ef9419a460d9700d7e1cf8ff071cf581e2ee4d6233"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
